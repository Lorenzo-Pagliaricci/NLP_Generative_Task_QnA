{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import difflib\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_FILLING = ''' \n",
    "                    You are an advanced language model that receives questions and must generate answers. \n",
    "                        For each request, produce a clear and synthetic answer, maximum one paragraph long.\n",
    "                        The answers must be direct and specific, without using bullet points or numbered lists.\n",
    "                '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_CSV = './data/eval_out.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(prompt, model, tokenizer):\n",
    "\tresponse = generate(model, tokenizer, prompt=prompt, verbose=False)\n",
    "\treturn response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "with open(\"data/processed/test.jsonl\", \"r\") as f:\n",
    "    test_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n",
      "Loading fine-tuned model...\n"
     ]
    }
   ],
   "source": [
    "print('Loading base model...')\n",
    "model_base, tokenizer_base = load('./models/base/Phi-3-mini-4k-instruct-4bit')\n",
    "\n",
    "print('Loading fine-tuned model...')\n",
    "model_ft, tokenizer_ft= load('./models/fused/fused_Phi-3-mini-4k-instruct-4bit_2bs_4ls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare OUT CSV\n",
    "with open(OUT_CSV, 'w', newline='') as csvfile:\n",
    "\tcsv_writer = csv.writer(csvfile)\n",
    "\t# Write the header\n",
    "\tcsv_writer.writerow([ 'Question Prompt', 'expected_answer', \n",
    "\t\t\t\t\t  \t'Base - Answer', 'base_similarity', 'bleu_base',\n",
    "\t\t\t\t\t\t'Finetuned - Answer', 'ft_similarity', 'bleu_ft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in test_data:\n",
    "    prompt = item['prompt']\n",
    "    expected_answer = item['completion']\n",
    "\n",
    "    filled_prompt_b = PROMPT_FILLING + prompt\n",
    "\n",
    "    answer_base = run_prompt(filled_prompt_b, model_base, tokenizer_base)\n",
    "    answer_ft = run_prompt(filled_prompt_b, model_ft, tokenizer_ft)\n",
    "\n",
    "    # Calcola la similarità tra la answer attesa e quelle generate\n",
    "    base_similarity = difflib.SequenceMatcher(None, expected_answer, answer_base).ratio()\n",
    "    ft_similarity = difflib.SequenceMatcher(None, expected_answer, answer_ft).ratio()\n",
    "\n",
    "    # Utilizzo del tokenizer del modello per una tokenizzazione coerente\n",
    "    # Per il modello base\n",
    "    expected_tokens_base = tokenizer_base.tokenize(expected_answer)\n",
    "    answer_base_tokens = tokenizer_base.tokenize(answer_base)\n",
    "\n",
    "    # Per il modello fine-tuned\n",
    "    expected_tokens_ft = tokenizer_ft.tokenize(expected_answer)\n",
    "    answer_ft_tokens = tokenizer_ft.tokenize(answer_ft)\n",
    "\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_base = sentence_bleu([expected_tokens_base], answer_base_tokens, smoothing_function=smoothing)\n",
    "    bleu_ft = sentence_bleu([expected_tokens_ft], answer_ft_tokens, smoothing_function=smoothing)\n",
    "\n",
    "    # Scrive i risultati nel CSV\n",
    "    with open(OUT_CSV, 'a', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow([\n",
    "            prompt, expected_answer,\n",
    "            answer_base, base_similarity, bleu_base,  # il dizionario verrà convertito in stringa\n",
    "            answer_ft, ft_similarity, bleu_ft])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
