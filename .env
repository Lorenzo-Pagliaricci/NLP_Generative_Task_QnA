# MODELLI
T5_BASE_223M='google-t5/t5-base'
T5_SMALL_60M='google-t5/t5-small'


T5_SMALL_60M_Q8='agkavin/t5-small-Q8_0-GGUF'

FLAN_T5_BASE_238M='google/flan-t5-base'
FLAN_T5_SMALL_77M='google/flan-t5-small'

BERT_BASE_110M='bert-base-uncased'

E5_SMALL_V2_33M='intfloat/e5-small-v2'

TinyMistral_248M='Locutusque/TinyMistral-248M'  # NO, non Ã¨ un Sequence to Sequence Model

M2M100_418M='cartesinus/iva_mt_wslot-m2m100_418M-en-pl'

# FABIO'S MODELc
BYT5_SMALL_300M='hmbyt5/byt5-small-english'
TEXT_GEN_77M='ayushparwal2004/text-gen-v1-small'
T5_SMALL_60M='Harshathemonster/t5-small-updated'

MODEL_NAME='BYT5_SMALL_300M'

PREPARED_DATASET='/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/NLP_Generative_Task_QnA/Transformers_Library/data/prepared_data'
TOKENIZED_DATASET='/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/NLP_Generative_Task_QnA/Transformers_Library/data/tokenized_data'
SAVED_MODEL_PATH='/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/NLP_Generative_Task_QnA/Transformers_Library/models'


# export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0

DATASET_HF_NAME='enelpol/rag-mini-bioasq'