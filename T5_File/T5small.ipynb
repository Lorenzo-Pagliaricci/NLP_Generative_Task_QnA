{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "!pip install -q transformers datasets evaluate scikit-learn pyarrow accelerate sentencepiece bitsandbytes"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import torch"
   ],
   "id": "f1ca8c5ecf69854"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Path to the QA dataset in Parquet format\n",
    "QA = \"Per_Creare_Dataset_Piccoli/DB_QC_A_da_utilizzare.parquet\"\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "qa_dataset = pd.read_parquet(QA)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "# 1. First, split the dataset into a training+validation set (90% of the data) and a test set (10% of the data)\n",
    "train_val, TEST_df = train_test_split(qa_dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "# 2. Then, split the training+validation set into training (80% of the remaining 90%) and validation (20% of the remaining 90%) sets\n",
    "# Note: The test size is adjusted to account for the original split (i.e., 20% of the 90%).\n",
    "TRAIN_df, VALIDATION_df = train_test_split(train_val, test_size=0.2 / (1 - 0.1), random_state=42)"
   ],
   "id": "bcd3a1ba534aafbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the tokenizer and base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ],
   "id": "7ec68033bf8d3d2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def tokenize_function(examples, tokenizer, max_input_length=512, max_target_length=128):\n",
    "    \"\"\"Tokenizes the dataset.\"\"\"\n",
    "\n",
    "    # Extract the 'input_text' (questions and contexts) and 'answer' columns from the examples\n",
    "    inputs = examples[\"input_text\"]\n",
    "    targets = examples[\"answer\"]\n",
    "\n",
    "    # Tokenize the input text (questions and contexts) with truncation and padding to a maximum length\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Tokenize the target text (answers) with truncation and padding to a maximum length\n",
    "    labels = tokenizer(text_target=targets, max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Add the tokenized target text (labels) to the model inputs dictionary\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs  # Return the tokenized inputs along with the labels\n",
    "\n",
    "# Prepare datasets by applying the tokenization function to each split\n",
    "train_dataset = Dataset.from_pandas(TRAIN_df).map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "val_dataset = Dataset.from_pandas(VALIDATION_df).map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "test_dataset = Dataset.from_pandas(TEST_df).map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "\n",
    "# Data collator to manage batching for sequence-to-sequence tasks\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ],
   "id": "7e4adeb5e755f715"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training arguments for the Seq2Seq model (e.g., T5)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='fine_tuned_model',  # Directory to save the fine-tuned model\n",
    "    evaluation_strategy=\"epoch\",  # Perform evaluation at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    learning_rate=3e-4,  # Optimal learning rate for T5 model\n",
    "    per_device_train_batch_size=8,  # Batch size for training per device (GPU/CPU)\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation per device\n",
    "    weight_decay=0.01,  # Regularization parameter to prevent overfitting\n",
    "    num_train_epochs=20,  # Total number of training epochs\n",
    "    predict_with_generate=True,  # Use model's `generate` method for prediction\n",
    "    generation_max_length=128,  # Maximum length of generated sequences (specific to T5)\n",
    "    fp16=torch.cuda.is_available(),  # Enable mixed precision training if CUDA is available\n",
    "    logging_steps=50,  # Log training metrics every 50 steps\n",
    "    save_total_limit=2,  # Keep only the last 2 checkpoints to save disk space\n",
    "    load_best_model_at_end=True,  # Load the best model (based on the evaluation loss) after training\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric used to determine the best model\n",
    "    greater_is_better=False,  # Whether a higher evaluation loss is better (False means lower is better)\n",
    "    report_to=\"none\",  # Disable reporting to external platforms like WandB\n",
    "    optim=\"adafactor\",  # Recommended optimizer for T5 (efficient in terms of memory and computation)\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps before updating model\n",
    "    group_by_length=True,  # Group sequences of similar lengths together to optimize batching\n",
    "    warmup_steps=100  # Number of warm-up steps for the learning rate scheduler\n",
    ")\n",
    "\n",
    "# Initialize the Seq2Seq trainer with the model, training arguments, dataset, tokenizer, and data collator\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,  # Pass the training arguments defined above\n",
    "    train_dataset=train_dataset,  # Training dataset\n",
    "    eval_dataset=val_dataset,  # Validation dataset\n",
    "    tokenizer=tokenizer,  # Tokenizer for encoding/decoding inputs and outputs\n",
    "    data_collator=data_collator,  # Data collator to handle dynamic padding\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4, early_stopping_threshold=0.001)]  # Early stopping to prevent overfitting\n",
    ")\n",
    "\n",
    "# Start the training process\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer to the specified output directory\n",
    "trainer.save_model('fine_tuned_model')\n",
    "tokenizer.save_pretrained('fine_tuned_model')\n"
   ],
   "id": "880c536d5b5bc19d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the tokenizer and the fine-tuned model\n",
    "tokenizer = AutoTokenizer.from_pretrained('fine_tuned_model')  # Load the tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('fine_tuned_model')  # Load the fine-tuned model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Check if GPU is available, otherwise use CPU\n",
    "model.to(device)  # Move the model to the appropriate device (GPU or CPU)\n",
    "\n",
    "# Convert the input data to PyTorch tensors for processing\n",
    "input_ids = torch.tensor(test_dataset[\"input_ids\"]).to(device)  # Convert the input IDs to tensors and move them to the correct device\n",
    "attention_mask = torch.tensor(test_dataset[\"attention_mask\"]).to(device)  # Convert attention mask to tensor and move it to the correct device\n",
    "\n",
    "# Generate predictions with the model (optimized for T5)\n",
    "model.eval()  # Set the model to evaluation mode (important for inference)\n",
    "with torch.no_grad():  # Disable gradient calculations to save memory during inference\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,  # The tokenized input data\n",
    "        attention_mask=attention_mask,  # The attention mask\n",
    "        max_length=128,  # Set the maximum length for the generated sequence\n",
    "        num_beams=4,  # Use beam search with 4 beams for better quality\n",
    "        early_stopping=True,  # Stop generation early if an end-of-sequence token is generated\n",
    "        length_penalty=0.6,  # Penalize shorter outputs to encourage longer, more complete answers\n",
    "        no_repeat_ngram_size=2,  # Avoid repeating n-grams (2-grams in this case)\n",
    "        temperature=0.7  # Controls the randomness of the generation (lower means more deterministic)\n",
    "    )\n",
    "\n",
    "# Decode the generated outputs into human-readable text\n",
    "generated_answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# Extract the original questions and contexts, removing the \"answer the question: \" prefix\n",
    "questions_contexts = [text.replace(\"answer the question: \", \"\") for text in test_dataset[\"input_text\"]]  # Clean up the input text\n",
    "real_answers = test_dataset[\"answer\"]  # Get the real answers from the test dataset\n",
    "\n",
    "# Display the model's output alongside the real answers for evaluation\n",
    "print(\"\\nTest of the T5_Model:\\n\")\n",
    "for i, (qc, gen, real) in enumerate(zip(questions_contexts, generated_answers, real_answers)):\n",
    "    if 10 and i >= 10:  # Limit to 10 examples for printing\n",
    "        break\n",
    "\n",
    "    # Print the input question/context, the generated answer, and the real answer\n",
    "    print(f\"### Example {i + 1} ###\")\n",
    "    print(f\"[Input]:\\n{qc}\")  # Print the input question/context\n",
    "    print(f\"\\n[Generated Answer]:\\n{gen}\")  # Print the generated answer\n",
    "    print(f\"\\n[Real Answer]:\\n{real}\")  # Print the actual real answer\n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\n\")  # Separator between examples\n"
   ],
   "id": "f6d816dc6a219abb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
