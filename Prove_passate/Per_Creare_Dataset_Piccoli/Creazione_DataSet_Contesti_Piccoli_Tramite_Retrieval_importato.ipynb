{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install pandas sentence-transformers spacy scikit-learn nltk torch\n",
    "!python -m download it_core_news_sm"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd  # Importing pandas for data manipulation\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util  # Importing models from sentence-transformers for sentence embeddings and reranking\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Importing TF-IDF vectorizer from scikit-learn for text feature extraction\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # Importing cosine similarity function to compute similarity between vectors\n",
    "from nltk.tokenize import sent_tokenize  # Importing NLTK's sentence tokenizer to split text into sentences\n",
    "from tabulate import tabulate  # Importing tabulate to display tables in a readable format\n",
    "import nltk  # Importing NLTK for various NLP tasks\n",
    "nltk.download('punkt')  # Downloading the 'punkt' tokenizer for sentence tokenization\n",
    "nltk.download('punkt_tab')  # Downloading additional NLTK data related to tokenization\n",
    "from nltk.tokenize import sent_tokenize  # Re-importing the sentence tokenizer from NLTK\n",
    "import torch  # Importing PyTorch for tensor operations (used in transformer models)\n",
    "\n",
    "# Load a pre-trained SentenceTransformer model for generating sentence embeddings\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Load a pre-trained CrossEncoder model for sentence-level ranking tasks (reranking)\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n"
   ],
   "id": "1deddd36588a5b6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def split_text(text, window=4, overlap=2):\n",
    "    # 1. Handle empty input and invalid parameters\n",
    "    if not text.strip():\n",
    "        return []  # Return empty list if the input text is empty\n",
    "\n",
    "    if window < 1 or overlap < 0 or overlap >= window:\n",
    "        raise ValueError(\"Invalid parameters: window > 0, 0 â‰¤ overlap < window\")\n",
    "\n",
    "    # 2. Robust sentence splitting for English using NLTK\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # 3. Optimization for very short texts (less than or equal to window size)\n",
    "    if len(sentences) <= window:\n",
    "        return [\" \".join(sentences)] if len(sentences) > 0 else []  # If text is short, return the entire text as a single chunk\n",
    "\n",
    "    # 4. Efficient chunking calculation using a sliding window approach\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while True:\n",
    "        end = start + window\n",
    "        chunk = sentences[start:end]\n",
    "\n",
    "        if not chunk:\n",
    "            break\n",
    "\n",
    "        chunks.append(\" \".join(chunk))  # Join sentences in each chunk to form a single string\n",
    "\n",
    "        if end >= len(sentences):\n",
    "            break\n",
    "\n",
    "        start += (window - overlap)  # Move start by window size minus overlap to ensure sliding window with overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Hybrid search (embedding + tfidf)\n",
    "def ricerca_ibrida(domanda, testo_completo, answer, top_k=5):\n",
    "    # 1. Split the complete text into chunks\n",
    "    chunks = split_text(testo_completo)\n",
    "\n",
    "    # 2. Filter chunks based on keywords from the answer\n",
    "    answer_keywords = set(nltk.word_tokenize(answer.lower()))  # Tokenize the answer and convert to lowercase\n",
    "    filtered_chunks = [\n",
    "        chunk for chunk in chunks\n",
    "        if any(keyword in chunk.lower() for keyword in answer_keywords)  # Keep chunks that contain any of the answer's keywords\n",
    "    ] if answer else chunks  # If no answer, use all chunks\n",
    "\n",
    "    # If no chunks are filtered, use all chunks\n",
    "    if not filtered_chunks:\n",
    "        filtered_chunks = chunks\n",
    "\n",
    "    # 3. Calculate embeddings for the question, answer, and filtered chunks\n",
    "    question_answer_embeddings = model.encode([domanda, answer], convert_to_tensor=True)  # Encoding question and answer\n",
    "    chunk_embeddings = model.encode(filtered_chunks, convert_to_tensor=True)  # Encoding filtered chunks\n",
    "\n",
    "    # 4. Compute semantic similarity between the question and the chunks\n",
    "    sim_domanda = util.pytorch_cos_sim(question_answer_embeddings[0], chunk_embeddings)[0].cpu().numpy()\n",
    "\n",
    "    # 5. Compute semantic similarity between the answer and the chunks\n",
    "    sim_risposta = util.pytorch_cos_sim(question_answer_embeddings[1], chunk_embeddings)[0].cpu().numpy()\n",
    "\n",
    "    # 6. Compute TF-IDF similarity\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2)).fit([domanda] + filtered_chunks)  # Fit TF-IDF vectorizer on question and chunks\n",
    "    tfidf_domanda = vectorizer.transform([domanda])  # Transform the question into its TF-IDF representation\n",
    "    tfidf_chunks = vectorizer.transform(filtered_chunks)  # Transform filtered chunks into their TF-IDF representations\n",
    "    punteggi_tfidf = cosine_similarity(tfidf_domanda, tfidf_chunks)[0]  # Compute cosine similarity between question and chunks\n",
    "\n",
    "    # 7. Combine the scores with specific weights\n",
    "    punteggi_combinati = (\n",
    "        0.5 * sim_domanda +  # Weight for the question-semantic similarity\n",
    "        0.3 * sim_risposta +  # Weight for the answer-semantic similarity\n",
    "        0.2 * punteggi_tfidf  # Weight for the TF-IDF similarity\n",
    "    )\n",
    "\n",
    "    # 8. Sort the chunks by their combined scores and select the top_k chunks\n",
    "    risultati_ordinati = sorted(zip(filtered_chunks, punteggi_combinati),\n",
    "                              key=lambda x: x[1], reverse=True)  # Sort by combined score in descending order\n",
    "\n",
    "    return [chunk for chunk, _ in risultati_ordinati[:top_k]]  # Return the top_k chunks\n",
    "\n",
    "\n",
    "# Final reordering function using a cross-encoder reranker\n",
    "def riordina_risultati(domanda, risultati):\n",
    "    coppie = [[domanda, risultato] for risultato in risultati]  # Create pairs of the question and each result\n",
    "    punteggi = reranker.predict(coppie)  # Use the cross-encoder to predict relevance scores for each pair\n",
    "    punteggi = torch.tensor(punteggi).cpu().numpy()  # Convert scores to numpy array for sorting\n",
    "    risultati_riordinati = [risultato for _, risultato in sorted(zip(punteggi, risultati), reverse=True)]  # Sort results by scores\n",
    "\n",
    "    return risultati_riordinati  # Return the reordered results\n"
   ],
   "id": "2b0df40e2a1a0980"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Creation of the new column 'Retrieval_Text'\n",
    "def crea_retrieval_text(row):\n",
    "    # 1. Split the input_text to separate the question and context\n",
    "    question_part, context_part = row['input_text'].split(\"\\nContext: \")  # Split the input_text by the context separator\n",
    "    question = question_part.split(\"Question: \")[1]  # Extract the question from the part after \"Question: \"\n",
    "    context = context_part  # The context comes after \"Context: \"\n",
    "\n",
    "    # 2. Retrieve the answer if it exists (default to an empty string if missing)\n",
    "    answer = row.get('answer', '')  # Safely get the answer using get()\n",
    "\n",
    "    # 3. Perform hybrid search for relevant sentences using the question, context, and answer\n",
    "    frasi_rilevanti = ricerca_ibrida(question, context, answer, top_k=10)  # Get top 10 relevant sentences using hybrid search\n",
    "\n",
    "    # 4. Reorder the retrieved sentences to prioritize the most relevant ones\n",
    "    frasi_riordinate = riordina_risultati(question, frasi_rilevanti)[:3]  # Get top 3 sentences after reranking\n",
    "\n",
    "    # 5. Format the final context string by combining the reordered sentences\n",
    "    final_context = f\"Context: {' '.join(frasi_riordinate)}\"\n",
    "\n",
    "    # 6. Return the final formatted text (question and context)\n",
    "    return f\"Question: {question} \\n{final_context}\"\n",
    "\n",
    "# Loading the dataset\n",
    "PATH = \"DB_QC_A_da_utilizzare.parquet\"  # Path to the Q&A dataset\n",
    "Q_A_DataSet = pd.read_parquet(PATH)  # Load the dataset into a pandas DataFrame\n",
    "\n",
    "# Create the new 'Retrieval_Text' column by applying the 'crea_retrieval_text' function row-wise\n",
    "Q_A_DataSet['Retrieval_Text'] = Q_A_DataSet.apply(lambda row: crea_retrieval_text(row), axis=1)\n",
    "\n",
    "# Print the first 5 rows of the dataset to check the new column\n",
    "print(tabulate(Q_A_DataSet.head(5), headers='keys', tablefmt='psql'))\n",
    "\n",
    "# 1. Remove the 'input_text' column as it's no longer needed\n",
    "Q_A_DataSet = Q_A_DataSet.drop(columns=['input_text'])\n",
    "print(tabulate(Q_A_DataSet.head(5), headers='keys', tablefmt='psql', showindex=False))\n",
    "# 2. Save the modified dataset with only the necessary columns into a new Parquet file\n",
    "Q_A_DataSet.to_parquet(\"DB_QC_A_retrieval.parquet\", index=False)  # Save the dataset as a Parquet file without row indices\n"
   ],
   "id": "c26b6630d7a075ab"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
