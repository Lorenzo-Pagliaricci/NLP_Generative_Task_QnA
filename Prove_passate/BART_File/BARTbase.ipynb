{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "!pip install -q transformers datasets evaluate scikit-learn pyarrow accelerate sentencepiece bitsandbytes"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import torch"
   ],
   "id": "9e06f50a6ab97feb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Path to the QA dataset in Parquet format\n",
    "QA = \"Per_Creare_Dataset_Piccoli/DB_QC_A_da_utilizzare.parquet\"\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "qa_dataset = pd.read_parquet(QA)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "# 1. First, split the dataset into a training+validation set (90% of the data) and a test set (10% of the data)\n",
    "train_val, TEST_df = train_test_split(qa_dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "# 2. Then, split the training+validation set into training (80% of the remaining 90%) and validation (20% of the remaining 90%) sets\n",
    "# Note: The test size is adjusted to account for the original split (i.e., 20% of the 90%).\n",
    "TRAIN_df, VALIDATION_df = train_test_split(train_val, test_size=0.2 / (1 - 0.1), random_state=42)"
   ],
   "id": "f0556f9aa1d90eb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the tokenizer and base model for BART (a seq2seq model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\", model_max_length=512)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")"
   ],
   "id": "4b2ed756074e9ae2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def tokenize_function(examples, tokenizer, max_input_length=512, max_target_length=128):\n",
    "    \"\"\"Tokenizes the dataset.\"\"\"\n",
    "\n",
    "    # Extract the 'input_text' (questions and contexts) and 'answer' columns from the examples\n",
    "    inputs = examples[\"input_text\"]\n",
    "    targets = examples[\"answer\"]\n",
    "\n",
    "    # Tokenize the input text (questions and contexts) with truncation and padding to a maximum length\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Tokenize the target text (answers) with truncation and padding to a maximum length\n",
    "    labels = tokenizer(text_target=targets, max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Add the tokenized target text (labels) to the model inputs dictionary\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs  # Return the tokenized inputs along with the labels\n",
    "\n",
    "\n",
    "\n",
    "# Prepare datasets by applying the tokenization function to each split\n",
    "train_dataset = Dataset.from_pandas(TRAIN_df).map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "val_dataset = Dataset.from_pandas(VALIDATION_df).map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "test_dataset = Dataset.from_pandas(TEST_df).map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "\n",
    "# Data collator to manage batching for sequence-to-sequence tasks\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ],
   "id": "aa37b8d5d2239438"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define training arguments for the Seq2Seq model\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='fine_tuned_model',  # Directory to save the fine-tuned model\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    load_best_model_at_end=True,  # Automatically load the best model after training\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric used to determine the best model (lower is better for eval_loss)\n",
    "    greater_is_better=False,  # For eval_loss, lower is better, so we set this to False\n",
    "    save_total_limit=2,  # Limit the number of saved models to 2 (older models will be deleted)\n",
    "    num_train_epochs=10,  # Set the number of training epochs to 10\n",
    "    per_device_train_batch_size=16,  # Batch size for training on each device (GPU or CPU)\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision (fp16) if a GPU with CUDA is available\n",
    "    logging_steps=50,  # Log training information every 50 steps\n",
    "    report_to=\"none\"  # No reporting to any external tool like WandB or TensorBoard\n",
    ")\n",
    "\n",
    "# Initialize the Seq2SeqTrainer with the model, training arguments, datasets, tokenizer, and callbacks\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,  # The model to train\n",
    "    args=training_args,  # The training arguments defined above\n",
    "    train_dataset=train_dataset,  # The training dataset\n",
    "    eval_dataset=val_dataset,  # The validation dataset\n",
    "    tokenizer=tokenizer,  # The tokenizer used during training\n",
    "    data_collator=data_collator,  # The collator that handles batching of data\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Early stopping callback with patience of 2 epochs\n",
    ")\n",
    "\n",
    "# Start training the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer to disk\n",
    "trainer.save_model('fine_tuned_model')  # Save the trained model\n",
    "tokenizer.save_pretrained('fine_tuned_model')  # Save the tokenizer\n"
   ],
   "id": "7bab9bdfc3c74050"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the tokenizer and model from the fine-tuned directory\n",
    "tokenizer = AutoTokenizer.from_pretrained('fine_tuned_model')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('fine_tuned_model')\n",
    "\n",
    "# Set the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move the model to the chosen device (GPU/CPU)\n",
    "\n",
    "# Prepare the tensors for input_ids and attention_mask from the test dataset\n",
    "input_ids = torch.tensor(test_dataset[\"input_ids\"]).to(device)\n",
    "attention_mask = torch.tensor(test_dataset[\"attention_mask\"]).to(device)\n",
    "\n",
    "# Set the model to evaluation mode (turn off dropout, etc.)\n",
    "model.eval()\n",
    "\n",
    "# Generate predictions (no gradients required during inference)\n",
    "with torch.no_grad():\n",
    "    # Generate the responses using beam search and early stopping\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=128,  # Maximum length of the generated response\n",
    "        num_beams=4,  # Beam search parameter (controls diversity of generated text)\n",
    "        early_stopping=True  # Stops generation when all beams reach the end\n",
    "    )\n",
    "\n",
    "# Decode the generated responses from token IDs to text\n",
    "generated_answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# Get the original questions and contexts from the test dataset\n",
    "questions_contexts = test_dataset[\"input_text\"]\n",
    "real_answers = test_dataset[\"answer\"]\n",
    "\n",
    "# Print the test results for inspection\n",
    "print(\"\\nModel Test Results:\\n\")\n",
    "for i, (qc, gen, real) in enumerate(zip(questions_contexts, generated_answers, real_answers)):\n",
    "    if 10 and i >= 10:  # Limit the output to 10 examples\n",
    "        break\n",
    "\n",
    "    # Print each example's question, generated answer, and real answer\n",
    "    print(f\"### Example {i + 1} ###\")\n",
    "    print(f\"[Question + Context]:\\n{qc}\")\n",
    "    print(f\"\\n[Generated Answer]:\\n{gen}\")\n",
    "    print(f\"\\n[Real Answer]:\\n{real}\")\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")"
   ],
   "id": "b7520b063d4f38d9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
