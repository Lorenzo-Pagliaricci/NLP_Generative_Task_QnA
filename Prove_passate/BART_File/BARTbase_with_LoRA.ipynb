{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "!pip install -q transformers datasets evaluate scikit-learn pyarrow accelerate sentencepiece bitsandbytes peft wandb nltk rouge_score\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import ast\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel, PeftConfig\n",
    "import evaluate\n",
    "import wandb\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "wandb.init(project=\"QA-FineTuning\", name=\"BART_File-LoRA-Experiment\")\n"
   ],
   "id": "6c1557f37750d39e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Path to the QA dataset in Parquet format\n",
    "QA = \"Per_Creare_Dataset_Piccoli/DB_QC_A_da_utilizzare.parquet\"\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "qa_dataset = pd.read_parquet(QA)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "# 1. First, split the dataset into a training+validation set (90% of the data) and a test set (10% of the data)\n",
    "train_val, TEST_df = train_test_split(qa_dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "# 2. Then, split the training+validation set into training (80% of the remaining 90%) and validation (20% of the remaining 90%) sets\n",
    "# Note: The test size is adjusted to account for the original split (i.e., 20% of the 90%).\n",
    "TRAIN_df, VALIDATION_df = train_test_split(train_val, test_size=0.2 / (1 - 0.1), random_state=42)\n"
   ],
   "id": "5f7c34ad6a0c15d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the tokenizer and base model for BART (a seq2seq model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\", model_max_length=512)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "# LoRA configuration (Low-Rank Adaptation)\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,  # Define the task as sequence-to-sequence language modeling\n",
    "    r=4,                             # Set the rank for the low-rank adaptation (a parameter controlling the amount of adaptation)\n",
    "    lora_alpha=32,                   # Scaling factor for LoRA weights\n",
    "    lora_dropout=0.1,                # Dropout rate for the LoRA layers (helps prevent overfitting)\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Specify which modules of the model to apply LoRA to (here it is the attention projection layers)\n",
    "    bias=\"none\",                     # Specify whether to include bias terms in the LoRA layers (here, no bias terms are used)\n",
    "    modules_to_save=[\"lm_head\", \"final_layer_norm\"]  # Specify which parts of the model to save after training (here, it's the language model head and the final layer normalization)\n",
    ")\n",
    "\n",
    "# Add LoRA to the model (apply the LoRA configuration to the base model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Display the trainable parameters of the model (useful to check which parameters are trainable after applying LoRA)\n",
    "model.print_trainable_parameters()\n"
   ],
   "id": "f1cfc2474903d907"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def tokenize_function(examples, tokenizer, max_input_length=512, max_target_length=128):\n",
    "    \"\"\"\n",
    "    Tokenizes the dataset.\n",
    "    Sets the truncation direction for inputs (left) and targets (right).\n",
    "    \"\"\"\n",
    "    # Set the truncation direction for inputs (truncates from the left side)\n",
    "    tokenizer.truncation_side = 'left'\n",
    "    inputs = examples[\"input_text\"]  # Extract the input text from the examples\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_length,  # Limit input length to 'max_input_length'\n",
    "        truncation=True,               # Enable truncation for longer inputs\n",
    "        padding=\"max_length\"           # Pad the inputs to 'max_length'\n",
    "    )\n",
    "\n",
    "    # Set the truncation direction for targets (truncates from the right side)\n",
    "    tokenizer.truncation_side = 'right'\n",
    "    targets = examples[\"answer\"]    # Extract the target (answer) from the examples\n",
    "    labels = tokenizer(\n",
    "        text_target=targets,         # Apply tokenizer on the target text (answer)\n",
    "        max_length=max_target_length, # Limit target length to 'max_target_length'\n",
    "        truncation=True,              # Enable truncation for longer targets\n",
    "        padding=\"max_length\"          # Pad the targets to 'max_length'\n",
    "    )\n",
    "\n",
    "    # Add the target tokenized labels to the model inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]  # Set the labels (target tokens) for model training\n",
    "    return model_inputs  # Return the tokenized input and label data\n",
    "\n",
    "# Prepare datasets by applying the tokenization function to each split\n",
    "train_dataset = Dataset.from_pandas(TRAIN_df).map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "val_dataset = Dataset.from_pandas(VALIDATION_df).map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "test_dataset = Dataset.from_pandas(TEST_df).map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "\n",
    "# Data collator to manage batching for sequence-to-sequence tasks\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
   ],
   "id": "aa19d4a16960d074"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to calculate metrics during evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred  # Extract predictions and labels from the evaluation results\n",
    "    # Decode the predictions\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 with the padding token in labels and decode\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    bleu_metric = evaluate.load(\"bleu\")  # Load the BLEU metric\n",
    "    bleu_results = bleu_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=[[label] for label in decoded_labels]  # Format labels as a list of lists for references\n",
    "    )\n",
    "\n",
    "    # Calculate ROUGE score\n",
    "    rouge_metric = evaluate.load(\"rouge\")  # Load the ROUGE metric\n",
    "    rouge_results = rouge_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels  # Use the decoded labels as the reference text\n",
    "    )\n",
    "\n",
    "    # Return the computed metrics (BLEU and ROUGE)\n",
    "    return {\n",
    "        \"bleu\": bleu_results[\"bleu\"],\n",
    "        \"rouge1\": rouge_results[\"rouge1\"],\n",
    "        \"rouge2\": rouge_results[\"rouge2\"],\n",
    "        \"rougeL\": rouge_results[\"rougeL\"]\n",
    "    }\n",
    "\n",
    "# Training arguments for the Seq2Seq model\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='fine_tuned_model',              # Directory where the fine-tuned model will be saved\n",
    "    evaluation_strategy=\"epoch\",                # Evaluate the model every epoch\n",
    "    save_strategy=\"epoch\",                      # Save the model every epoch\n",
    "    learning_rate=2e-4,                         # Learning rate for training\n",
    "    per_device_train_batch_size=2,              # Batch size for training\n",
    "    per_device_eval_batch_size=4,               # Batch size for evaluation\n",
    "    num_train_epochs=20,                        # Number of training epochs\n",
    "    weight_decay=0.01,                          # Weight decay for regularization\n",
    "    save_total_limit=2,                         # Limit the number of saved checkpoints\n",
    "    predict_with_generate=True,                 # Required for prediction during evaluation\n",
    "    fp16=torch.cuda.is_available(),             # Enable mixed-precision training if GPU with fp16 support is available\n",
    "    load_best_model_at_end=True,                # Load the best model at the end of training\n",
    "    metric_for_best_model=\"bleu\",               # Metric used to determine the best model\n",
    "    greater_is_better=True,                     # Whether higher values of the metric are better\n",
    "    report_to=\"wandb\"                           # Log metrics to W&B (Weights and Biases)\n",
    ")\n",
    "\n",
    "# Initialize the Seq2SeqTrainer with model, arguments, and datasets\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                               # The model to train\n",
    "    args=training_args,                        # Training arguments defined above\n",
    "    train_dataset=train_dataset,               # Training dataset\n",
    "    eval_dataset=val_dataset,                  # Validation dataset\n",
    "    tokenizer=tokenizer,                       # Tokenizer for encoding text\n",
    "    data_collator=data_collator,               # Data collator to handle batching\n",
    "    compute_metrics=compute_metrics,           # Function to compute evaluation metrics\n",
    "    callbacks=[                                # List of callbacks\n",
    "        EarlyStoppingCallback(                 # Callback to stop training early if no improvement\n",
    "            early_stopping_patience=10,         # Number of epochs with no improvement to wait\n",
    "            early_stopping_threshold=0.01       # Minimum improvement to consider as progress\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Measure the training time (it can take up to an hour or more depending on the model and dataset size)\n",
    "start_time = time.time()\n",
    "trainer.train()  # Start training\n",
    "training_duration = time.time() - start_time  # Calculate the total training time\n",
    "print(f\"Training duration: {training_duration/60:.2f} minutes\")  # Print the training duration in minutes\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained('fine_tuned_model')  # Save the model\n",
    "tokenizer.save_pretrained('fine_tuned_model')  # Save the tokenizer\n"
   ],
   "id": "a6ee01cda271dd80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the LoRA configuration and base model\n",
    "config = PeftConfig.from_pretrained('fine_tuned_model')  # Load the LoRA configuration from the saved model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)  # Load the base model\n",
    "\n",
    "# Load the LoRA weights\n",
    "model = PeftModel.from_pretrained(model, 'fine_tuned_model')  # Apply LoRA weights to the model\n",
    "model = model.merge_and_unload()  # Merge the LoRA weights with the base model for inference (unloads LoRA-related parameters)\n",
    "\n",
    "# Load the tokenizer used during training\n",
    "tokenizer = AutoTokenizer.from_pretrained('fine_tuned_model')\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move the model to the selected device (GPU/CPU)\n",
    "\n",
    "# Prepare the tensors for the test set (input_ids and attention_mask)\n",
    "input_ids = torch.tensor(test_dataset[\"input_ids\"]).to(device)  # Convert input_ids to tensor and move to device\n",
    "attention_mask = torch.tensor(test_dataset[\"attention_mask\"]).to(device)  # Convert attention_mask to tensor and move to device\n",
    "\n",
    "# Generate responses with the desired decoding parameters\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computation to speed up inference\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,  # Input tensor\n",
    "        attention_mask=attention_mask,  # Attention mask tensor\n",
    "        max_length=200,  # Maximum length of the generated output\n",
    "        min_length=5,  # Minimum length of the generated output (ensures the answer is not too short)\n",
    "        num_beams=8,  # Beam search parameter for controlling diversity in the generation process\n",
    "        length_penalty=0.8,  # Length penalty to favor longer answers\n",
    "        no_repeat_ngram_size=2,  # Prevents repetition of n-grams during generation\n",
    "        early_stopping=True,  # Stops the generation early if all beams end the generation\n",
    "        do_sample=False,  # Perform deterministic decoding (no sampling)\n",
    "        temperature=0.5,  # Controls randomness in generation (lower value makes it more deterministic)\n",
    "        top_k=50,  # Top-K sampling parameter, controls the diversity of generated tokens\n",
    "        top_p=0.95  # Top-p sampling parameter (nucleus sampling) for diversity\n",
    "    )\n",
    "\n",
    "# Decode the generated answers (convert token IDs to readable text)\n",
    "generated_answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# Extract the actual answers from the test set\n",
    "questions_contexts = test_dataset[\"input_text\"]  # Extract input text (questions + context)\n",
    "real_answers = test_dataset[\"answer\"]  # Extract real answers from the test set\n",
    "\n",
    "# Compute the evaluation metrics using the 'evaluate' library\n",
    "bleu_metric = evaluate.load(\"bleu\")  # Load BLEU metric\n",
    "test_bleu = bleu_metric.compute(\n",
    "    predictions=generated_answers,  # Generated answers\n",
    "    references=[[real] for real in real_answers]  # Real answers (formatted as a list of lists for references)\n",
    ")\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")  # Load ROUGE metric\n",
    "rouge_results = rouge_metric.compute(\n",
    "    predictions=generated_answers,  # Generated answers\n",
    "    references=real_answers  # Real answers (single list format for ROUGE)\n",
    ")\n",
    "\n",
    "# Print the evaluation metrics (BLEU and ROUGE)\n",
    "print(f\"\\nTest BLEU: {test_bleu['bleu']}\")  # Print BLEU score\n",
    "print(f\"Test ROUGE-L: {rouge_results['rougeL']}\")  # Print ROUGE-L score\n",
    "\n",
    "# Log the metrics to Weights and Biases (WandB) for tracking the experiment\n",
    "wandb.log({\n",
    "    \"test_bleu\": test_bleu[\"bleu\"],\n",
    "    \"test_rougeL\": rouge_results[\"rougeL\"],\n",
    "    \"test_rouge1\": rouge_results[\"rouge1\"],\n",
    "    \"test_rouge2\": rouge_results[\"rouge2\"]\n",
    "})\n",
    "\n",
    "# Print some example inputs and outputs for manual inspection\n",
    "print(\"\\nModel Test Examples:\\n\")\n",
    "for i, (qc, gen, real) in enumerate(zip(questions_contexts, generated_answers, real_answers)):\n",
    "    if 20 and i >= 20:  # Limit to 20 examples for display\n",
    "        break\n",
    "    print(f\"### Example {i + 1} ###\")\n",
    "    print(f\"[Question + Context]:\\n{qc}\")\n",
    "    print(f\"\\n[Generated Answer]:\\n{gen}\")\n",
    "    print(f\"\\n[Real Answer]:\\n{real}\")\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")  # Separator line for readability\n"
   ],
   "id": "57469972c7b237f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
