{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/NLP_Generative_Task_QnA/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import difflib\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_FILLING = ''' \n",
    "                    You are an advanced language model that receives questions and must generate answers. \n",
    "                        For each request, produce a clear and synthetic answer, maximum one paragraph long.\n",
    "                        The answers must be direct and specific, without using bullet points or numbered lists.\n",
    "                '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_CSV = './data/eval_out.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(prompt, model, tokenizer):\n",
    "\tresponse = generate(model, tokenizer, prompt=prompt, verbose=False)\n",
    "\treturn response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "with open(\"data/processed/test.jsonl\", \"r\") as f:\n",
    "    test_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n",
      "Loading fine-tuned model...\n"
     ]
    }
   ],
   "source": [
    "print('Loading base model...')\n",
    "model_base, tokenizer_base = load('./models/base/Phi-3-mini-128k-instruct-4bit')\n",
    "\n",
    "print('Loading fine-tuned model...')\n",
    "model_ft, tokenizer_ft= load('./models/fused/fused_Phi-3-mini-128k-instruct-4bit_2bs_4ls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare OUT CSV\n",
    "with open(OUT_CSV, 'w', newline='') as csvfile:\n",
    "\tcsv_writer = csv.writer(csvfile)\n",
    "\t# Write the header\n",
    "\tcsv_writer.writerow([ 'Question Prompt', 'expected_answer', \n",
    "\t\t\t\t\t  \t'Base - Answer', 'base_similarity', 'bleu_base',\n",
    "\t\t\t\t\t\t'Finetuned - Answer', 'ft_similarity', 'bleu_ft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m filled_prompt_b \u001b[38;5;241m=\u001b[39m PROMPT_FILLING \u001b[38;5;241m+\u001b[39m prompt\n\u001b[1;32m      7\u001b[0m answer_base \u001b[38;5;241m=\u001b[39m run_prompt(filled_prompt_b, model_base, tokenizer_base)\n\u001b[0;32m----> 8\u001b[0m answer_ft \u001b[38;5;241m=\u001b[39m \u001b[43mrun_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilled_prompt_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_ft\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Calcola la similaritÃ  tra la answer attesa e quelle generate\u001b[39;00m\n\u001b[1;32m     11\u001b[0m base_similarity \u001b[38;5;241m=\u001b[39m difflib\u001b[38;5;241m.\u001b[39mSequenceMatcher(\u001b[38;5;28;01mNone\u001b[39;00m, expected_answer, answer_base)\u001b[38;5;241m.\u001b[39mratio()\n",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m, in \u001b[0;36mrun_prompt\u001b[0;34m(prompt, model, tokenizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_prompt\u001b[39m(prompt, model, tokenizer):\n\u001b[0;32m----> 2\u001b[0m \tresponse \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/NLP_Generative_Task_QnA/.venv/lib/python3.10/site-packages/mlx_lm/generate.py:685\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model, tokenizer, prompt, verbose, formatter, **kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    684\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 685\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m stream_generate(model, tokenizer, prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    687\u001b[0m         \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/NLP_Generative_Task_QnA/.venv/lib/python3.10/site-packages/mlx_lm/generate.py:618\u001b[0m, in \u001b[0;36mstream_generate\u001b[0;34m(model, tokenizer, prompt, draft_model, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m detokenizer\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    617\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, (token, logprobs, from_draft) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(token_generator):\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    620\u001b[0m         prompt_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m tic\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/NLP_Generative_Task_QnA/.venv/lib/python3.10/site-packages/mlx_lm/generate.py:607\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    605\u001b[0m     token_generator \u001b[38;5;241m=\u001b[39m generate_step(prompt, model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;66;03m# from_draft always false for non-speculative generation\u001b[39;00m\n\u001b[0;32m--> 607\u001b[0m     token_generator \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    608\u001b[0m         (token, logprobs, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m token, logprobs \u001b[38;5;129;01min\u001b[39;00m token_generator\n\u001b[1;32m    609\u001b[0m     )\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    611\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_kv_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/NLP_Generative_Task_QnA/.venv/lib/python3.10/site-packages/mlx_lm/generate.py:374\u001b[0m, in \u001b[0;36mgenerate_step\u001b[0;34m(prompt, model, max_tokens, sampler, logits_processors, max_kv_size, prompt_cache, prefill_step_size, kv_bits, kv_group_size, quantized_kv_start, prompt_progress_callback)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m!=\u001b[39m max_tokens:\n\u001b[1;32m    373\u001b[0m     next_y, next_logprobs \u001b[38;5;241m=\u001b[39m _step(y)\n\u001b[0;32m--> 374\u001b[0m     \u001b[43mmx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masync_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_logprobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    376\u001b[0m     mx\u001b[38;5;241m.\u001b[39meval(y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for item in test_data:\n",
    "    prompt = item['prompt']\n",
    "    expected_answer = item['completion']\n",
    "\n",
    "    filled_prompt_b = PROMPT_FILLING + prompt\n",
    "\n",
    "    answer_base = run_prompt(filled_prompt_b, model_base, tokenizer_base)\n",
    "    answer_ft = run_prompt(filled_prompt_b, model_ft, tokenizer_ft)\n",
    "\n",
    "    # Calcola la similaritÃ  tra la answer attesa e quelle generate\n",
    "    base_similarity = difflib.SequenceMatcher(None, expected_answer, answer_base).ratio()\n",
    "    ft_similarity = difflib.SequenceMatcher(None, expected_answer, answer_ft).ratio()\n",
    "\n",
    "    # Utilizzo del tokenizer del modello per una tokenizzazione coerente\n",
    "    # Per il modello base\n",
    "    expected_tokens_base = tokenizer_base.tokenize(expected_answer)\n",
    "    answer_base_tokens = tokenizer_base.tokenize(answer_base)\n",
    "\n",
    "    # Per il modello fine-tuned\n",
    "    expected_tokens_ft = tokenizer_ft.tokenize(expected_answer)\n",
    "    answer_ft_tokens = tokenizer_ft.tokenize(answer_ft)\n",
    "\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_base = sentence_bleu([expected_tokens_base], answer_base_tokens, smoothing_function=smoothing)\n",
    "    bleu_ft = sentence_bleu([expected_tokens_ft], answer_ft_tokens, smoothing_function=smoothing)\n",
    "\n",
    "    # Scrive i risultati nel CSV\n",
    "    with open(OUT_CSV, 'a', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow([\n",
    "            prompt, expected_answer,\n",
    "            answer_base, base_similarity, bleu_base,  # il dizionario verrÃ  convertito in stringa\n",
    "            answer_ft, ft_similarity, bleu_ft])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
